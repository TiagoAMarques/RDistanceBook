---
title: A crash course in generalized additive models
bibliography: full.bib
csl: biometrics.csl
animation: true
---

```{r echo=FALSE, include=FALSE}
## abstract this into a header
source("../figure-captions.R")
library(animation)
opts_chunk$set(cache=TRUE, echo=FALSE)
```

In this chapter we'll take a tour through the world of generalized additive models (GAMs) and see how these flexible models work in practice. Note that although this chapter gives a broad overview of GAMs, it is no substitute for a comprehensive understanding of the framework. I highly recommend CITE WOOD 2017 both as a starting point and a useful reference for more complex modelling techniques. That book alone is a few hundred pages, what I can condense into a chapter here is the bare bones in comparison.

With that in mind, here we'll look into some of the mathematics behind generalized additive models while keeping an eye on the practical implications that these technical details have on modelling biological populations.


- What is a GAM?
- What is smoothing?
- How do GAMs work?
- Fitting GAMs using `dsm`


## What is a GAM?

Before trying to start to use GAMs we need to understand what they are really *doing*. We saw in the previous chapter an "equation for our model" but what does that really *mean*? Fundementally understanding what the model is doing is key to successful modelling -- this sounds obvious, but it's surprisingly common for folks to not understand what their model does, asking yourself questions like this can be very helpful during the modelling process.

We'll start thinking about GAMs[^altgam] by dissecting the term "generalized additive model":

- Generalized: is in the same sense as "generalized linear model", it means that we can use many response distributions to model the data (we are not just restricted to normal distributions).
- Additive: the terms in our model add together.
- Models: of course, we have a model.


### Response distributions

Generalized refers to the response distribution. Let's think about that response to begin with. As we talked about in the previous chapter, the response for a DSM is a count or estimated abundance in a given segment. I'll use the term "count" here very generally for the rest of this section, but I mean both. These counts should be modelled using a count distribution (the classic count distribution is Poisson, but we'll come across other, more flexible families later).

Now what do we mean by "should be modelled using a count distribution"? What we're really talking about is modelling the mean (or expected value) of that count distribution. We want to say that given particular covariate values, we will expect a particular value of the distribution on average. We model the relationship between the counts and the covariates using the smooths (or other additive terms), which we'll get to in the next section.



There are some complications of modelling counts, we need to think about the requirements we need for a useful response distribution. Figure XXXX shows a histogram of the counts per segment for the Gulf of Mexico dolphin data. We can observe some important (and common) features of the data:

- *Counts are mostly zero*. Though this can differ based on the species, observaers and land/seascape, one should expect that the majority of the segments to have zero counts.
- *Overdispersion*: For our cannonical count distribution, the Poisson, we assume that the mean and variance are equal. This is clearly not the case if we have many zero counts and then a series of non-zero (perhaps large) counts. To model this kind of data, we require flexible mean-variance relationships -- the three dsitributions we look at below, have this property.

```{r countshist, fig.width=5, echo=FALSE, message=FALSE, fig.cap=""}
library(dsm)
data(mexdolphins)
# just grab the non-zero count samples
# awkward aggregate step
count_data <- aggregate(obsdata$size, list(obsdata$Sample.Label), sum)
names(count_data) <- c("Sample.Label", "count")
dat <- merge(segdata, count_data, by="Sample.Label", all.x=TRUE)
dat$count[is.na(dat$count)] <- 0

hist(dat$count, xlab="Count", main="", breaks=c(0,10,20,50,100,200,400,700),
     freq=TRUE)
```

Generally speaking, the response is a count but is not not always integer. This can be because we used the estimated abundance as the response (which doesn't guarantee that the reponse will be whole numbers of animals) but can also be because we averaged group sizes from multiple observations (many multi-observer cruises average group sizes as estimated by different members of the observer team to get a more robust estimate). Our response should take this into account (this means that the Poisson distribution cannot be used, as it will only accept integer values).

We're going to focus on using three response distributions here, they are the: quasi-Poisson, Tweedie and the negative binomial distribution.



```{r tweedie-nb, fig.width=10, echo=FALSE, message=FALSE, fig.cap="Tweedie and negative binomial distributions"}
library(tweedie)
library(RColorBrewer)

par(mfrow=c(1,2))

# tweedie
y<-seq(0.01,5,by=0.01)
pows <- seq(1.2, 1.9, by=0.1)

fymat <- matrix(NA, length(y), length(pows))

i <- 1
for(pow in pows){
  fymat[,i] <- dtweedie( y=y, power=pow, mu=2, phi=1)
  i <- i+1
}

plot(range(y), range(fymat), type="n", ylab="Density", xlab="Count", cex.lab=1.5,
     main="Tweedie")

rr <- brewer.pal(8,"Dark2")

for(i in 1:ncol(fymat)){
  lines(y, fymat[,i], type="l", col=rr[i], lwd=2)
}

# Negative binomial
y<-seq(1,12,by=1)
disps <- seq(0.001, 1, len=10)

fymat <- matrix(NA, length(y), length(disps))

i <- 1
for(disp in disps){
  fymat[,i] <- dnbinom(y, size=disp, mu=5)
  i <- i+1
}

plot(range(y), range(fymat), type="n", ylab="Density", xlab="Count", cex.lab=1.5,
     main="Negative binomial")

rr <- brewer.pal(8,"Dark2")

for(i in 1:ncol(fymat)){
  lines(y, fymat[,i], type="l", col=rr[i], lwd=2)
}
```

#### Quasi-Poisson

The quasi-Poisson "distribution" is not really a distribution but is a quick way of fitting things that have particular mean-variance relationships. The quasi-Poisson assumes only that $\text{Var}\left(\text{count}\right) = \phi\mathbb{E}(\text{count})$ where $\phi$ is referred to as a *dispersion parameter* that scales the mean appropriately so that the variance is large enough. This leads to a more flexible modelling option than the Poisson distribution, but still only allows for linear scaling.

The quasi-Poisson distribution's quasi nature is a bit of a drawback in practice. As quasi distributions don't have probability density functions, it's rather difficult to do some of the model checking that we'd really like to do for these models. On the other hand quasi-Poisson models are usually quick to put together and can be a good first step to building a model.


#### Tweedie

The Tweedie distribution (CITE XXXX) is not a single distribution at all, but rather a series of different distributions that one can obtain by setting a specific parameter (that we'll refer to here as the "power parameter"). Tweedie incorporates Poisson, Gamma and Normal distributions and things inbetween.

Tweedie distributions are often referred to in the statistical literature as "gamma mixtures of Poisson random variables", while this may seem a little opaque it is a useful way of thinking about them... Intuitively we can think of our survey (ship or person or plane) travelling to a given segment, at this segment, we either see something or we don't (gamma distributed) and if we do see something, we see a given count (Poisson distributed).

The power parameter, $q$, dictates which distribution we get and also tells us what the mean-variance relationship will be. The mean-variance relationship is given by: $\text{Var}\left(\text{count}\right) = \phi\mathbb{E}(\text{count})^q$, where $\phi$ is a scale parameter (XXXX ref to quasi above). Setting $q=1 \Rightarrow$ Poisson, $q=2 \Rightarrow$ Gamma and $q=3 \Rightarrow$ Normal. Once $q$ gets below $1.2$, we see some odd behaviour from the distribution (we get a multimodal distribution, which seems unrealistic. We are only interested in distributions between $1.2 < q < 2$ and really there is not much difference in the distributions if we vary $q$ at below the first decimal place (so we really only need to think about $q = 1.2, 1.3, \ldots, 1.9$, as shown in figure XXXX).

#### Negative binomial

- $\text{Var}\left(\text{count}\right) =$ $\mathbb{E}(\text{count}) + \kappa \mathbb{E}(\text{count})^2$
- Estimate $\kappa$
- Is quadratic relationship a "strong" assumption?
- Similar to Poisson: $\text{Var}\left(\text{count}\right) =\mathbb{E}(\text{count})$ 


### Additive terms

We talk about "additive terms" above and in the previous chapter we started thinking about "smooths" and "wiggles". Now let's get into a bit more detail with these terms and think about how to really model the covariate effects in our DSMs.

Let's start by thinking about the good old statistical workhorse, the linear model. Figure XXXX shows a plot of some data, the left panel shows a linear fit through the data, the right shows a linear fit (`y~x`) with an additional term: the explanatory variable squared (`y~x+poly(x, 2)`). This seems to give a good fit too. The true function that generates this data is $\exp(2x)$, so the latter seems like a better approximation.

```{r islinear, fig.width=10}
library(ggplot2)
library(gridExtra)
set.seed(2) ## simulate some data...
dat <- gamSim(1, n=400, dist="normal", scale=1, verbose=FALSE)
dat <- dat[,c("y", "x0", "x1", "x2", "x3")]


p <- ggplot(dat,aes(y=y,x=x1)) +
  geom_point() +
  theme_minimal()
grid.arrange(p + geom_smooth(method="lm"),
             p + geom_smooth(method="lm", formula=y~x+poly(x, 2)),
             ncol=2)
```

This "trick" of adding polynomial terms seems to work, and could be extended to higher order terms, but is it sustainable? In short: no. First, it feels somewhat *ad hoc* to add terms like this, the choice of which terms to add seems somwhat arbitrary and although it's easy for simple examples like the one above, knowing which relationships to choose for more complex relationships is down to the expertise of the modeller. Second, the polynomials are defined over the whole of the range of the covariate, one can't restrict their effect to a subset of the range. So if the line we need to fit looks like pieces of different functions, fitting a model with simple polynomials won't work. Third, even if we want the extra terms to effect the whole range of the covariate, these (simple) polynomials are not terribly efficient at doing this -- they lack the mathematical property of *orthogonality* and tend to "overlap" in their effects, so each additional term doesn't add something new[^orthogonal]. Fourth, there is no control over how additional polynomials should effect the model fit, there is capacity for the model to overfit, making the model less general and useful. With these points in mind, wouldn't it be nice to think about adding wiggles to our models through a framework rather than *ad hoc* additions?

What if we could introduce flexible wiggles into our models, built using simple locally-acting functions that add together to create complicated effects? Enter those $s$ terms we saw in the previous chapter, which we'll refer to here as *smooths* (or *smoothers*) or *splines* (more technically). Splines take a simple idea: you can build complicated things from lots of simple things and apply this to making wiggles in your models. Splines are formulated as:
$$
s(x) = \sum_{k=1}^K \beta_k b_k(x),
$$
where we estimate $K$ $\beta_k$ parameters which multiply the *basis functions* $b_k$. We'll ignore the exact form of the basis functions for now, but bear in mind that there are many modelling options when it comes to which basis to use, and they can be tailored to the specific modelling task. Figure XXXX shows some basis functions (dashed) and how they can add (when scaled by appropriate $\beta_k$s) to create another function (solid line).

```{r basis-ex, results='hide', fig.width=10, fig.width=5, fig.cap="Eight basis functions (grey, dashed lines) used to build a function (solid line)."}
# generate some data
set.seed(2)
dat <- gamSim(1, n=400, dist="normal", scale=2)

# fit a model
b <- gam(y~s(x0, k=8, bs="cr"), data=dat)

# main plot
plot(b, se=FALSE, rug=FALSE, ylim=c(-1, 1), lwd=2, asp=1/2,
     xlab="x", ylab="s(x)")

# extract coefficients
cf <- coef(b)
# build an Lp matrix
xp <- data.frame(x0=seq(0, 1, length.out=100))
Xp <- predict(b, newdata=xp, type="lpmatrix")

# plot each basis function
for(i in 1:length(cf)){
  cf_c <- cf
  cf_c[-i] <- 0
  cf_c[i] <- 1
  lines(xp$x0, as.vector(Xp%*%cf_c), lty=2, lwd=1.5, col="grey")
}
```


### How do we estimate smooths?



```{r ruhroh, fig.width=12, fig.height=6}
p <- ggplot(dat, aes(y=y, x=x2)) + geom_point() +
      theme_minimal()
print(p + geom_smooth(method="lm", formula=y~x+poly(x, 2)))
```



```{r wiggles}
library(mgcv)
# hacked from the example in ?gam
set.seed(2) ## simulate some data... 
dat <- gamSim(1,n=50,dist="normal",scale=0.5, verbose=FALSE)
dat$y <- dat$f2 + rnorm(length(dat$f2), sd = sqrt(0.5))
f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10-mean(dat$y)
ylim <- c(-4,6)

# fit some models
b.justright <- gam(y~s(x2),data=dat)
b.sp0 <- gam(y~s(x2, sp=0, k=50),data=dat)
b.spinf <- gam(y~s(x2),data=dat, sp=1e10)

curve(f2,0,1, col="blue", ylim=ylim)
points(dat$x2, dat$y-mean(dat$y))
```

- Want a line that is "close" to all the data
- Don't want interpolation -- we know there is "error"
- Balance between interpolation and "fit"

Measuring wigglyness
======================

- Visually:
  - Lots of wiggles == NOT SMOOTH
  - Straight line == VERY SMOOTH
- How do we do this mathematically?
  - Derivatives!
  - (Calculus *was* a useful class afterall)



Wigglyness by derivatives
==========================

```{r wigglyanim, results="hide"}
#library(numDeriv)
#f2 <- function(x) 0.2*x^11*(10*(1-x))^6+10*(10*x)^3*(1-x)^10 - mean(dat$y)
#
#xvals <- seq(0,1,len=100)
#
#plot_wiggly <- function(f2, xvals){
#
#  # pre-calculate
#  f2v <- f2(xvals)
#  f2vg <- grad(f2,xvals)
#  f2vg2 <- unlist(lapply(xvals, hessian, func=f2))
#  f2vg2min <- min(f2vg2) -2
#
#  # now plot
#  for(i in 1:length(xvals)){
#    par(mfrow=c(1,3))
#    plot(xvals, f2v, type="l", main="function", ylab="f")
#    points(xvals[i], f2v[i], pch=19, col="red")
#
#    plot(xvals, f2vg, type="l", main="derivative", ylab="df/dx")
#    points(xvals[i], f2vg[i], pch=19, col="red")
#
#    plot(xvals, f2vg2, type="l", main="2nd derivative", ylab="d2f/dx2")
#    points(xvals[i], f2vg2[i], pch=19, col="red")
#    polygon(x=c(0,xvals[1:i], xvals[i],f2vg2min),
#            y=c(f2vg2min,f2vg2[1:i],f2vg2min,f2vg2min), col = "grey")
#
#    ani.pause()
#  }
#}
#
#saveGIF(plot_wiggly(f2, xvals), "wiggly.gif", interval = 0.2, ani.width = 800, ani.height = 400)
```

![Animation of derivatives](wiggly.gif)

Making wigglyness matter
=========================

- Integration of derivative (squared) gives wigglyness
- Fit needs to be **penalised**
- **Penalty matrix** gives the wigglyness 
- Estimate the $\beta_k$ terms but penalise objective
  - "closeness to data" + penalty

Penalty matrix
===============

- For each $b_k$ calculate the penalty
- Penalty is a function of $\beta$
  - $\lambda \beta^\text{T}S\beta$
- $S$ calculated once
- smoothing parameter ($\lambda$) dictates influence

Smoothing parameter
=======================


```{r wiggles-plot, fig.width=15}
# make three plots, w. estimated smooth, truth and data on each
par(mfrow=c(1,3), cex.main=3.5)

plot(b.justright, se=FALSE, ylim=ylim, main=expression(lambda*plain("= just right")))
points(dat$x2, dat$y-mean(dat$y))
curve(f2,0,1, col="blue", add=TRUE)

plot(b.sp0, se=FALSE, ylim=ylim, main=expression(lambda*plain("=")*0))
points(dat$x2, dat$y-mean(dat$y))
curve(f2,0,1, col="blue", add=TRUE)

plot(b.spinf, se=FALSE, ylim=ylim, main=expression(lambda*plain("=")*infinity)) 
points(dat$x2, dat$y-mean(dat$y))
curve(f2,0,1, col="blue", add=TRUE)

```

How wiggly are things?
========================

- We can set **basis complexity** or "size" ($k$)
  - Maximum wigglyness
- Smooths have **effective degrees of freedom** (EDF)
- EDF < $k$
- Set $k$ "large enough"

Okay, that was a lot of theory...
==================================
type:section

Translating maths into R
==========================

$$
n_j = A_j\hat{p}_j \exp\left[ \beta_0 + s(\text{y}_j) \right] + \epsilon_j
$$

where $\epsilon_j \sim N(0, \sigma^2)$, $\quad n_j\sim$ count distribution

- inside the link: `formula=count ~ s(y)`
- response distribution: `family=nb()` or `family=tw()`
- detectability: `ddf.obj=df_hr`
- offset, data: `segment.data=segs, observation.data=obs` 


(`method="REML"` uses REML to select the smoothing parameter)

`dsm` is based on `mgcv` by Simon Wood

Plotting

```{r plotsmooth}
#plot(dsm_x_tw)
```

- `plot(dsm_x_tw)`
- Dashed lines indicate +/- 2 standard errors
- Rug plot
- On the link scale
- EDF on $y$ axis


Adding a term
===============

- Just use `+`
```{r xydsm, echo=TRUE}
#dsm_xy_tw <- dsm(count ~ s(x) + s(y),
#                 ddf.obj=df_hr,
#                 segment.data=segs, observation.data=obs,
#                 family=tw(), method="REML")
```



Bivariate terms

- Assumed an additive structure
- No interaction
- We can specify `s(x,y)` (and `s(x,y,z,...)`)

Thin plate regression splines

- Default basis
- One basis function per data point
- Reduce # basis functions (eigendecomposition)
- Fitting on reduced problem
- Multidimensional


Bivariate spatial term

```{r xy-biv-dsm, echo=TRUE}
#dsm_xyb_tw <- dsm(count ~ s(x, y),
#                 ddf.obj=df_hr,
#                 segment.data=segs, observation.data=obs,
#                 family=tw(), method="REML")
```


```{r visgam, echo=TRUE}
#vis.gam(dsm_xyb_tw, view=c("x","y"), plot.type="contour", too.far=0.1, asp=1)
```

- Still on link scale
- `too.far` excludes points far from data

Comparing bivariate and additive models
========================================

```{r xy-x-y, fig.width=15}
#dsm_xy_nb <- dsm(count~s(x,y),
#                 ddf.obj=df_hr,
#                 segment.data=segs, observation.data=obs,
#                 family=nb(), method="REML")
#dsm_x_y_nb <- dsm(count~s(x) +s(y),
#                  ddf.obj=df_hr,
#                  segment.data=segs, observation.data=obs,
#                  family=nb(), method="REML")
#par(mfrow=c(1,2))
#vis.gam(dsm_xy_nb, plot.type = "contour", view=c("x","y"), zlim = c(-11,1), too.far=0.1, asp=1, main="Bivariate")
#vis.gam(dsm_x_y_nb, plot.type = "contour", view=c("x","y"), zlim = c(-11,1), too.far=0.1, asp=1, main="Additive")
```


Basis size (k)
===========

- Set `k` per term
- e.g. `s(x, k=10)` or `s(x, y, k=100)`
- Penalty removes "extra" wigglyness
  - *up to a point!*
- (But computation is slower with bigger `k`)

Checking basis size
====================

```{r gamcheck-text, fig.keep="none", echo=TRUE}
#gam.check(dsm_x_tw)
```


Increasing basis size
====================

```{r gamcheck-kplus-text, fig.keep="none", echo=TRUE}
#dsm_x_tw_k <- dsm(count~s(x, k=20), ddf.obj=df_hr,
#                  segment.data=segs, observation.data=obs,
#                  family=tw(), method="REML")
#gam.check(dsm_x_tw_k)
```

Sometimes basis size isn't the issue...
========================================

- Generally, double `k` and see what happens
- Didn't increase the EDF much here
- Other things can cause low "`p-value`" and "`k-index`"
- Increasing `k` can cause problems (nullspace)

### Putting it all together






## Recap




## Further reading

- @VerHoef:2007gx compare quasi-Poisson and negative binomial models.

## References

[^altgam]: Natalie Kelly (Australian Antarctic Division) pointed me to another definition of "gam", as seen in Moby Dick. 1. Collective noun used to refer to a group of whales, or rarely also of porpoises; a pod or, 2. (by extension) A social gathering of whalers.
[^orthogonal]: To get around this we could use a different set of polynomials, like the Hermite polynomials we saw as an adjustment term for detection functions, but they will still act globally over the whole covariate range.
