# Data setup for density surface models

**Pasted from todo list**:

- resolution
- possible data sources
- measuring effort
   - going on/off transect
   - 1 vs 2 sided transects
   - points?




This section deals with the practical issues involved in creating a dataset that can be analysed using `dsm`.

In general we need three pieces of information, these are outlined here then followed by a series of concrete examples at the end of the section showing how to setup the data in various situations.

 * *segment data* - think of this as the details of the survey. This `data.frame` gives the information on each segment in the survey. This must include a unique identifier for each segment and some measure of the effort expended.
 * *`ddf` object* - this is the object returned from fitting the detection function to the distance data. The probabilities of detection per observation are stored here and used by DSM to correct the counts/estimate the effective areas of the transects.
 * *observation data* - this `data.frame` serves as a link between the `ddf` object and the segment data, allowing the observations to be allocated to segments via a look-up. This might be the same `data.frame` that was passed to fit the `ddf` model.


## Segment size and effort


### Segment size

 * does segment size matter that much? Probably want to go into more detail than just citing Steve's sentence on this. Maybe construct an argument about this?

1) How would one test whether environmental variables varied appreciably within a segment?

Calculating the CV per-segment based on the finest resolution data available probably isn't a bad way of dealing with this.




(TKTKTK
 * EFFORT: temporal replicates? Should they be new samples (increase n) or effort multipliers (increase effort).
)

    Dear Ujjal,
    
    Thanks for your e-mail. This is a relatively common problem and is somewhat dependent on what you want to get out of the analysis and how the field procedure was conducted.
    
    You correctly state the two options:
    
     * each segment is 2km, you visit them three times, so this could give three unique segments in the data (three rows in the segment data.frame). If the segments are relatively spaced out and you expect the distribution of the animals to have changed between the sampling occasions then this seems more reasonable.
    
     * each segment is 2km, you visit them three times, so just make the effort for each segment 2km*3 (one row with the effort column multiplied by 3). I like this less for two reasons: 1) if the surveys are spaced in time then there will be movement between survey occasions, so the distribution will change and you model is then based on some kind of average over time, not counts and 2) you sort of falsely report your sample size since you did sample three times but you're pretending that the sample size is 1/3 of what it actually was (this in turn will probably lead to higher variance estimates).
    
    So, I would opt for including each segment three times: one for each survey occasion. Do remember to make the Sample.Label for the segments unique though. For example for transect 2, segment 4 on occasion 2 you might name it: "2-4-2", rather than including "2-4" three times.
    
    On 01/09/2013 09:24, Ujjwal Kumar wrote:> Dear Dr. David
    > I am doing an analysis of ungulate abundance in DSM framework to predict 
    > surface density. My Line transects are of 10Km line length. I walked 3 
    > day per transect that is pseudo replicates(temporal replicates) 
    > resulting 30km effort per transect. It is easy to make 30km effort for 
    > each transect and analyse data in Distance program, But when it comes to 
    > DSM package where we have to give spatial information for each segment. 
    > I am confused! please help me in this regard. I have segmented  10Km 
    > transects into 5 segments of 2 Km  length. So what should I use in 
    > segment effort 2Km or 2Km x 3day effort= 6 km, but my segment spatial 
    > information is of 2 km only.????
    > I shall be very thankful to you.
    > -- 
    > with regards
    > Ujjwal Kumar
    > / Research Fellow/
    >   P.O. Box 18 Chandrabani
    > Dehradun (Uttarakhand) 248001
    > mobile: +91 9808712591(Dehradun)
    >                  +91 8103228214 (Kanha



## A word about GIS and shapefiles

[[TKTKTK does this belong in an appendix?]]

Many of the tasks that need to be performed to build the segment and observation data are much more easily performed in GIS. Most GIS packages have built-in tools for chopping transects into segments and for finding which observations lie in which segments.

The output format from GIS is often a shapefile or a comma separated value file. (CSV). CSV files are much easier to read into R and are generally more portable however, often one is sent shapefiles and has to deal with them. The R packages [`sp`](http://cran.r-project.org/web/packages/sp/index.html), [`maptools`](http://cran.r-project.org/web/packages/maptools/index.html) and [`rgdal`](http://cran.r-project.org/web/packages/rgdal/index.html) are useful for manipulating shapefiles. Much has been written about both GIS and shapefiles, which I do not intend to re-iterate. Barry Rowlingson has a [useful cheatsheet for spatial data in R](http://www.maths.lancs.ac.uk/~rowlings/Teaching/UseR2012/cheatsheet.html). I'll note a couple of tips here when using shapefiles:

 * Once a shapefile has been loaded in R, it has a number of "slots": `@points`/`@lines`/`@polygons`, `@data`, `@proj`, etc. The `@points`/`@lines`/`@polygons` include information on the locations of the points or lines or polygons in the data. These locations each have a row assigned to them in  `@data`, but there is not validation between the two, so the locations in `@data` may not correspond to those in the `@points`/`@lines`/`@polygons` (which will be used if one calls `plot` on the object). Checking this can avoid some nasty issues later.
 * The locations in the shapefile will be according to some projection of geographic coordinates. Again, there is much literature about this but it's worth making sure that an appropriate projection is used (TKTKTK citations here?). Projections can be changed using the `spTransform` function in `rgdal` ([spatialreference.org](http://spatialreference.org/) provides information about projection strings; see also [this useful SO thread](http://gis.stackexchange.com/questions/31743/projecting-sp-objects-in-r)).
 * Usually projecting the data into a suitable coordinate system and measuring distances as (kilo)metres from a particular feature (e.g. the centre of the study area) makes locations much easier to deal with (and also makes two-dimensional smoothing easier; see [Smoothing] (TKTKTK references between chapters?)).
 * Using an incorrect projection (especially in the far North and South) can cause very big problems in modelling. (TKTKTK reference to this)


## Obtaining additional covariates

Often environmental covariates that may be useful in the analysis were not collected at the same time as observations of the species in question. There are several large databases of geographically (and temporally) referenced data available. Although it would be impossible to list all such repositories here, some of the more popular databases are listed.

 * [NOAA Environmental Research Division's Data Access Program](http://coastwatch.pfeg.noaa.gov/erddap/index.html) (ERDDAP) -- contains sea surface temperature, chlorophyll-a levels, wind speeds, bathymetry and other variables for the coastlines of the USA.
 * [North Atlantic Oscillation](http://www.cru.uea.ac.uk/~timo/datapages/naoi.htm) -- data from University of East Anglia on pressure differenced between the Azores and Iceland, thought to be a good indicator of Northern Hemisphere climate.
 * [British Oceanographic Data Centre: Numerical model data](https://www.bodc.ac.uk/data/online_delivery/numerical_model_data/request/) -- similar to ERDDAP for UK coastline.
 * [British Oceanographic Data Centre](https://www.bodc.ac.uk/data/online_delivery/gebco/) -- bathymetry data for the UK.
 * [marineexplore.org](http://marineexplore.org) -- a large repository of worldwide oceanographic data from a large number of governmental and academic sources.
 * [LANDSAT data](https://earth.esa.int/web/guest/data-access/browse-data-products/-/asset_publisher/y8Qb/content/landsat-5-thematic-mapper-geolocated-terrain-corrected-systematic-processing-over-kiruna) -- landscape classification data

(TKTKTK more here?)

There are several R packages that allow one to easily import data from these databases:

 * [ROpenSci](https://github.com/ropensci/) -- offer a series of packages that give access to large databases.
 * `marmap` -- NOAA ETOPO1 bathymetry and tpolograph  http://www.plosone.org/article/info:doi%2F10.1371%2Fjournal.pone.0073051

TIP: Having included these variables in the data set, it is worth checking that the values fall in the range that you thought they did (using `range`) and mapping the covariates to ensure that they have been downloaded correctly and allocated to the segments/grid points that you expect. In particular it can be useful to download the values your require for prediction grid at the same time and plot these to check that the distribution is as expected. Additionally, keeping track of the units and scales that covariates are measured on can save confusion in the interpretation of the results.

## Examples

### Loons

(TKTKTK This is the easy data set).


### Dolphins

### Bears

(TKTKTK More complicated because the segments that the bears were observed from were not the same as the ones where covariates were measured.)

## References

 * []


