---
title: Models for detectability
bibliography: full.bib
csl: biometrics.csl
---

```{r echo=FALSE}
## abstract this into a header
source("../figure-captions.R")
```

What became clear in the previous chapter is that when we model the effect of detectability in our surveys we can reliably estimate abundance. The previous examples were simulations, so we knew the truth; it's now time to get our hands dirty with some survey data.

Let's begin by looking at data collected on pantropical spotted dolphins (*Stenella attenuata*) in the Gulf of Mexico[^gomdata]. Data was collected by NOAA's Southeast Fisheries Science Centre and comes from several shipboard surveys in the Gulf of Mexico during 1996 as part of the GulfCet II Program. 47 groups of pantropical spotted dolphins were observed by the crew of the Oregon II.

We can start by loading the `Distance2` package which contains both the data we want to analyse and the functions we'll use later.
```{r load-datapkg}
library(Distance2)
data(mexdolphins)
```
For now we'll ignore the exact format of the data (which is simply a `data.frame`, we'll investigate it's columns in the next chapter), for now we'll concentrate on the `distance` column. Looking at a histogram of the distances, as we did previously:
```{r mex-dolphin-plot, fig.cap="Histogram of observed distances to pantropical spotted dolphins in the Gulf of Mexico from the South East Fisheries Science Center's GoMex Oceanic 1996 survey."}
hist(mexdolphins$distance, main="", xlab="Distance (m)")
```
As with the data we simulated in the previous chapter, we can see that there is a decrease in frequency with increasing distance from the observer (though this is a little less well behaved in reality). If the assumption of strip transect sampling were true this histogram would be flat, indicating that we had seen everything out to the furthest distance.

## Detection functions

We now want to model the decrease in the histogram of distances in order to correct the counts we have, and go on to estimate abundance.

The relationship between count frequency and distance is described by a *detection function*. More formally, we model the probability that we detect an animal given that it is at a certain distance ($x$, say) from the line, mathematically:
$$
\mathbb{P}(\text{animal detected } \vert \text{ animal at distance }x)
$$
In general, we'll refer to the detection function as $g$, which for now we'll just say is a function of the distances ($x$) and its parameter(s) (generally $\theta$ or $\boldsymbol{\theta}$), which we'll estimate.

In the previous chapter, we looked at a *half-normal detection function*:
$$
g(x; \sigma)=\exp\left(-\frac{x^2}{2\sigma^2}\right),
$$
here we denote the parameter we estimate as $\sigma$, which we call the *scale parameter* as it changes the scale of the function (but not it's shape). We can half-normal detection functions defined over $(0,1)$ with some different scale parameters below:
```{r hn-scale-par-comp, fig.width=10, fig.height=3, fig.cap="Half-normal detection functions with varying scale parameter ($\\sigma$ values are given above the plots). Note that the shape of the detection function stays the same in each plot, as the scale increases. From left to right, the study species becomes more detectable (higher probability of detection at larger distances).", echo=FALSE}
par(mfrow=c(1,4))
g.hn <- function(x,sigma) exp(-x^2/(2*sigma^2))
x <- seq(0,1,len=200)
for(this.sig in c(0.05, 0.25, 1, 10)){
  plot(x, g.hn(x,this.sig), main=bquote(sigma == .(this.sig)),
       xlab="Distance", ylab="Detection probability",
       type="l", xlim=c(0,1), ylim=c(0,1), asp=1)
}
```

We'd like to have the shape of the detection function mimic the shape of the histogram; we'll go into more detail on how that's done below but for now let's try to fit a detection function to the Gulf of Mexico dolphin data...
```{r fit-dolphins-hn}
hn.df <- ds(mexdolphins, truncation=8000)
```
The function `ds` will fit the detection function for us (to data set `mexdolphins`), we set the `truncation` to be 8000m and fit a simple half-normal model (the default, other options are covered in Chapter 4). Let's see what that looks like before we examine the model in further detail:
```{r dolphins-hn-plot, fig.cap="Half-normal detection function (solid black line) fitted to the Gulf of Mexico dolphin observed distances (histogram)."}
plot(hn.df, main="")
```

As with most R objects, we can use the `summary` function to find out further information about our fitted detection function:
```{r dolphins-hn-summary}
summary(hn.df)
```
The summary tells us how many observations were used to fit the data, the truncation distance, what detection function we used, the AIC, parameter estimates and finally the "`Average p`" and "`N in covered region`" (along with uncertainty). Before we go on to talk about these last two important items, it is first worth noting that the parameter estimate (marked `(Intercept)`) is on the log scale, so $\sigma=\exp$ `r round(hn.df$par,2)` (why the parameters are reported in this way will become apparent in the next chapter).

The half-normal is not the only option for modelling $g$ (indeed, there are many options as we will see in the next chapter). Another possibility is the *hazard-rate detection function*, which is more flexible, but requires the estimation of an extra parameter. The plot below shows examples of hazard-rate detection functions.
```{r hr-par-comp, fig.width=10, fig.height=3, fig.cap="Hazard-rate detection function with varying scale and shape parameters (given above each plot). Compared to the half-normal, the hazard-rate as a more prominent \"shoulder\".", echo=FALSE}
par(mfrow=c(1, 4))
g.hr <- function(x, sigma, b) 1 - exp(-(x/sigma)^-b)
x <- seq(0, 1, len=200)
for(this.sig in c(0.1, 0.5)){
  for(this.b in c(5, 1)){
    plot(x, g.hr(x, this.sig, this.b),
         main=bquote(sigma == .(this.sig) ~ .("b") == .(this.b)),
         xlab="Distance", ylab="Detection probability",
         type="l", xlim=c(0,1), ylim=c(0,1), asp=1)
  }
}
```
Mathematically the hazard-rate detection function is defined as:
$$
g(x; \sigma, b)=1-\exp\left(-\left(\frac{x}{\sigma}\right)^{-b}\right).
$$
The hazard-rate detection function has two parameters: $\sigma$, the scale parameter as in the half-normal and $b$, which is a *shape* parameter. The role of the shape parameter is to control the size of the *shoulder* of the detection function -- how far detection remains certain. We can see it's effect in comparing between plots where $b=5$ and $b=1$, a larger $b$ gives a longer distance out to which we observe animals with certainty.

We can fit a hazard-rate detection function to the dolphin data in a similar way to half-normal, though now we must specify a detection function model:
```{r fit-dolphins-hr}
hr.df <- ds(mexdolphins, truncation=8000, model=df(model=~hr))
```
we can then plot this, as before:
```{r dolphins-hr-plot, fig.cap="Hazard-rate detection function (solid black line) fitted to the Gulf of Mexico dolphin observed distances (histogram)."}
plot(hr.df)
```
We can again use the `summary` function:
```{r dolphins-hr-summary}
summary(hr.df)
```
Note that there are now two rows of parameters, one for scale and then one for shape.

The hazard-rate model has a rather different average detection probability from the half-normal model (`r round(predict(hr.df)[1],4)` rather than `r round(predict(hn.df)[1],4)` for the half-normal) and hence rather different abundance estimates. How do we decide between these two models?

## What makes a good detection function?

In general, we'd like to fit a number of different detection functions, choose the best and use that for our subsequent inference. In order to choose which detection function is "best", we must first define what we mean by "best". One can view this as a three stage process:

  1. amongst the candidate models, which are "reasonable",
  2. perform goodness of fit testing on the reasonable models,
  3. amongst those reasonable, well fitting models, which is the best.

### Reasonable models

First we need to ensure that the models that we are comparing are "reasonable". What I mean by this is they pass the following checks:

  * **Biologically reasonable**: It's important, before fitting models, to ensure that models are biologically plausible. Rexstad and coauthors [-@Rexstad:1988vz] show that one can easily construct datasets that apparently contain "useful" predictors but are actually total garbage. Although their example is deliberately silly, it highlights an important principle: avoiding a modelling philosophy of "throw everything in and see what happens". This will be particularly pertinent in the next chapter, when we talk about factors other than distance that affect detectability.
  * **Convergent**: It is sometimes the case that when fitting a model, an warning will be reported about the optimisation not having converged. In this case the results are not necessarily reliable. We'll cover troubleshooting such models in Chapter **XXX**, offering some ways to fix non-convergent models. For now, let's just say that it's generally a bad idea to include such models in the model selection process.
  * **Eyeball test**: Inspecting fitted model plots can tell you a lot. One should not underestimate the amount of information that can be gleaned from simply plotting the detection function. It is possible that a model can converge but with rather "wacky" parameters leaving a model that doesn't resemble the data at all. Ensuring that such models are investigated (again, more in this in Chapter **XXX**) and excluded is another important step towards model selection.


### Goodness of fit testing

Three statistical tests are commonly used to assess goodness of fit for detection functions, these are: $\chi^2$, Kolmogorov-Smirnov and Cramer-von Mises tests. The latter two tests are based on quantifying the results of quantile-quantile plots (Q-Q plots). Goodness of fit testing involves measuring discrepancies in how the model describes the data and the data themselves. Goodness of fit tests give **absolute** measure of model fit.


#### Q-Q plots

A Q-Q plot allows us to graphically assess how similar samples from two distributions are. To judge goodness of fit for detection functions, we are interested in comparing the cumulative distribution function (CDF) and empirical distribution function (EDF). The CDF (of the detection function) evaluates the probability of observing an animal at a distance less than or equal to a given value ($x$, say); we usually denote the CDF as $F(x)$. The empirical distribution function gives the proportion of observations which have evaluations of the CDF less than or equal to a given value (here we evaluate $F(x)$ where we set $x$ to be the observed distances). In other words, we're judging whether the number of observations less than or equal to a given value are in line with what the model says they should be. The "given values" that we use are the observed distances. If our model fits well then we should have agreement between these two measures so plotting them against each other should lead to a straight line at $y=x$. Points above the line indicate that the model thinks there should be more observations than there were, points below the line indicate that there were more observations than the model thinks.

We can plot a Q-Q plot for our fitted detection functions using the `gof_tests` function in `Distance2`:
```{r dolphin-hn-gof, fig.cap="Quantile-quantile plot for the half-normal detection function fitted to the dolphin data."}
gof_tests(hn.df)
```
The function also prints out some statistics for the two tests we'll investigate next. First let's look at the plot. The grey line indicates the $y=x$ line, we can see that there seem to be rather more points below that line than above, this indicates that we may have a problem with our model fit. To further investigate this we need to formalise what we mean by "far from the line $y=x$", which we'll achieve by looking at two statistical tests.


#### Kolmogorov-Smirnov test

We can see from the above Q-Q plot that there are some deviations from the line $y=x$. The Kolmogorov-Smirnov test uses this idea and asks the question "what's the largest vertical distance between a point and the $y=x$ line?" It uses this distance as it's test statistic to test the null hypothesis that the samples (EDF and CDF in our case) are from the same distribution (and hence our model fits well). If the deviation between the $y=x$ line and the points is too large we reject the null hypothesis and say the model doesn't have a good fit. The $p$-value for the test is given in the `gof_tests` output and is `r round(gof_tests(hn.df)$kolmogorov_smirnov$p, 4)`, telling us we should not reject the null hypothesis, this deviation from the line is tolerable.


#### Cramer-von Mises test

Rather than looking at the single biggest difference between the $y=x$ line and the points in the Q-Q plot, we might prefer to think about all the differences between line and points, since there may be many smaller differences that we want to take into account rather than looking for one large deviation. In this case the Cramer-von Mises test can help us. Its null hypothesis is the same, but sum the distances from each of the point to the line. For the half-normal detection function we can see above that the $p$-value (of `r round(gof_tests(hn.df)$cramer_vonMises$p, 4)`) also doesn't indicate that we should reject the null hypothesis.

To illustrate these ideas further, we can perform the goodness of fit testing for the hazard-rate model:
```{r dolphin-hr-gof}
gof_tests(hr.df, plot=FALSE)
```
using the `plot=FALSE` argument, we can prevent the plot from being shown and just give the test statistics and $p$-values. The output for the hazard-rate model gives us weak evidence that we should reject our hypothesis that the distributions of the CDF and EDF are the same. The Cramer-von Mises test is more powerful (since we are using more information in calculating its statistic), so we should usually pay it more attention.

The next plot shows the Q-Q plot for the hazard-rate model, with the Kolmogorov-Smirnov test statistic highlighted in blue and the differences used in the calculation of the Cramer-von Mises statistic highlighted in red.

```{r dolphin-hr-gof-plot, echo=FALSE, fig.cap="Quantile-quantile plot for the hazard-rate detection function fitted to the dolphin data. The blue line is the Kolmogorov-Smirnov test statistic and the red lines are used in the calculation of the Cramer-von Mises statistic."}
gof_tests(hr.df)
# from gof_tests internals
gof_tests_statplot <- function(model){
  int_f <- model$df_obj$integrate_df
  # calculate the CDF values
  cdf_values <- rep(NA, nrow(model$data))
  for(i in 1:nrow(model$data)){
    this_data <- model$data[i,, drop=FALSE]
    cdf_values[i] <- int_f(0, model$data$distance[i], model$df_obj, model$par,
                           data=this_data)
  }
  # integrate over full range and divide by that to get CDF
  pdf_normalisation <- int_f(0, model$df_obj$truncation$right, model$df_obj,
                             model$par, data=model$data)
  # divide through and sort
  cdf_values <- sort(cdf_values/pdf_normalisation)

  # calculate EDF values -- number of values less than or equal to each
  #  value
  edf_values <- vapply(cdf_values, function(x,y) sum(y<=x), y=cdf_values,
                       numeric(1))/length(cdf_values)

  # plot Cramer-von Mises distances
  Map(function(edf, cdf){
        lines(x=rep(edf,2), y=c(edf, cdf), lwd=1.5, col="red")
      },
      edf_values, cdf_values)

  # find & plot which line-point distance is the test statistic for K-S test
  ks.ind <- which.max(abs(cdf_values-edf_values))
  lines(x=rep(edf_values[ks.ind],2), y=c(edf_values[ks.ind], cdf_values[ks.ind]), lwd=1.5, col="blue")


  invisible()
}
gof_tests_statplot(hr.df)
```

The results from the goodness of fit tests show that neither of our proposed detection functions lead to a particularly poor fit. We therefore go on to think about comparing the models to each other.

*$\chi^2$ tests have been omitted here. They're most appropriate for binned data, so we'll come back to them when we look at binning distances in Chapter **XXX**.*


### Model selection by AIC

Once we have a reasonable set of models, we want to be able to rank them in order of how well they represent the data. It is relatively easy to ensure that the model fits the data too well (as we can make our model for the detection function arbitrarily complex and reproduce the histogram of distances). To avoid this, we can penalise our measure of model fit based on the number of parameters we use -- making us justify the use of the extra complexity by an appropriately-sized payoff in fit. Akaike's Information Criterion (AIC) is the metric that we'll use to perform model selection for the detection function. This ensures that we pick models that represent the data well while ensuring we have parsimonious models. In contrast to goodness of fit testing, AIC only gives a **relative** measure of model fit.

We see the AIC reported for the two models above in their summaries. We can also access the AIC via the `$AIC` element of the model object:
```{r aic-element}
hn.df$AIC
hr.df$AIC
```
A smaller AIC is better, but if the AICs are closer than 2 and differ by only one parameter (as we see in the two detection functions above), we can discard the more complex model as it only marginally decreases the AIC at the cost of one additional parameter: hardly parsimonious. Based on this, we can discard the hazard-rate model, leaving the half-normal as our chosen detection function at this point.

At this point it's worth noting that we're never interested in the absolute values of the AIC, but rather the different between the best candidate model's AIC and the others. Articles will often include tables with a column labelled $\Delta$AIC containing these differences. Burnham and Anderson [-@Burnham:2002tk] is a thorough treatment of AIC in an ecological modelling context, as well as a more philosophical guide on statistical modelling.

# Model outputs

Having selected the half-normal model, we can return to the `summary` results above. The last two rows of the summary report statistics about `Average p` and `Nhat in covered region`, but what is the meaning of these two quantities?

## Average detectability

With a fitted detection function we can evaluate the probability of observing an animal given its distance but, what we'd rather know is the probability of detecting an animal on average (since our aim is to make inference about the population of animals, rather than a single observation in particular). To do this we can simply integrate out distance from the detection function -- removing the conditioning on the distance. Denoting the average probability of detection as $\hat{P_a}$, we can write:
$$
\hat{P_a} = \frac{1}{w}\int_0^w g(x;\boldsymbol{\theta}) \text{d}x
$$
where $w$ denotes the truncation distance (which we set to 8000m). For the pantropical dolphins we obtain $\hat{P_a}=$ `r round(predict(hn.df)[1],4)` (see also the output of the `summary`, under `Average p`/`Estimate`).


## Abundance in covered area

Although the average probability of detection is an interesting quantity in its own right, we are primarily interested in estimating abundance. The row `Nhat in covered area` gives an estimate of the number of animals in the covered area (that is, the area we actually surveyed -- the area we covered by transects out to the truncation distance). To compute this abundance estimate (which we'll denote $\hat{N}$) we use a *Horvitz-Thompson-like estimator* [@Thompson:2002wi], which one can think of as "correcting" the size of the observations based on the probability of detection. That is, if we saw 1 dolphin in this study, we should actually inflate this by $1/\hat{P_a}$.

The Horvitz-Thompson estimator is defined as:
$$
\hat{N} = \sum_{i=1}^n \frac{s_i}{\hat{P_a}},
$$
where $s_i$ is the size of the group of the $i^\text{th}$ observation. In the dolphin data group size ranges from `r min(mexdolphins$size)` to `r max(mexdolphins$size)`. We can look at a histogram of the observed distances:

```{r dolphins-size-hist, fig.width=5, fig.height=5, fig.cap="Histogram of observed group sizes of dolphins in the Gulf of Mexico. Most groups are in the size range 15-200, but there are a few much larger groups, these have a much larger effect on the Hovitz-Thompson estimator."}
hist(mexdolphins$size, main="", xlab="Observed group size", breaks=seq(0, 700, by=25))
```

The Horvitz-Thompson-like estimator above is used in the `summary` of the model so calculate the abundance in the covered area. We may also want to extrapolate and estimate abundance in a larger area outside where we covered (though still within reasonable geographic/environmental limits of the area covered by the surveys). In the simplest case, we can simply divide our estimate of $\hat{N}$ through by the covered area (twice the truncation multiplied by length of transects, multiplied by the average detectability, $2wL\hat{P_a}$), however we'll cover this in a little more detail and subtlety in the next chapter.



# Point transects

So far we've looked at the dolphin data from the Gulf of Mexico, which consists of line transects. Now let's look at point transect data and show that fitting a detection function to that data and estimating abundance is not significantly different.

We'll be looking at data consisting of 1485 observations of amakihi (*Hemignathus virens*), a bird species native to Hawaii[^amakihi]. As for the dolphins we can again load the data, which is bundled with `Distance2`:
```{r amakihi-data}
data(amakihi)
```
Plotting the histogram of observed distances:
```{r amakihi-plot, fig.cap="Histogram of observed distances to amakihi observations."}
hist(amakihi$distance, main="", xlab="Distance (m)")
```
The histogram is the shape we saw in the previous chapter, an increase up to a point (due to the geometry of the sampler), then a drop off after that point. We also notice that unlike the dolphin example, the observed distances trail off rather more gradually, with a few distances higher than 200m (the median observed distance is `r round(median(amakihi$distance),2)`. We don't really want to model the long tail in the distances as they are likely outliers, so we will *truncate* the data. Before we do let's see what happens when we don't truncate.

```{r amakihi-notrunc}
hn.df.notrunc <- ds(amakihi, transect="point")
```
Notice two differences here in our call to `ds`: we supply `transect="point"` to tell `ds` that our model is of point transect data and that we didn't include a `truncation`. When we don't supply a truncation distance `ds` uses the largest observed distance and warns us that is happening.

We can again look at summary information for the fitted model
```{r amakihi-notrunc-summary}
summary(hn.df.notrunc)
```
from the summary, we can see that we estimate $P_a=$`r round(predict(hn.df.notrunc)[1],4)` and plot the resulting model along with goodness of fit information:
```{r amakihi-notrunc-plot, fig.width=9, fig.cap="Plots of a half-normal detection function fitted to the amakihi point transect data. Left: the fitted detection function with histogram of distances (rescaled according to the change in area due to point transect sampler geometry); the detection function drops to zero at around 150m. Right: quantile-quantile plot, showing that the model does not fit well (many dots below the line $y=x$)."}
par(mfrow=c(1,2))
plot(hn.df.notrunc, xlab="Distance (m)", main="")
gof_tests(hn.df.notrunc)
```
We can see that many points lie below the line $y=x$, indicating that the model thinks there should be fewer observations up to each distance than were observed (the $p$-values of the goodness of fit tests also reflect this). This, combined with the detection function going to zero well before the truncation (largest observed distance was `r round(max(amakihi$distance),2)`m), at a little less than 150m. Note that the histogram plotted with the detection function here has been rescaled to take into account the increasing area searched with increasing radial distance. We can plot the fitted model's probability density function with the unscaled histogram by supplying the `pdf=TRUE` option to `plot` (note that this has no effect on the shape of the plot for line transect data/models):
```{r amakihi-notrunc-plot-pdf, fig.cap="Plot of the probability density function for the model fitted to the amakihi data with histogram of observed distances."}
plot(hn.df.notrunc, xlab="Distance (m)", pdf=TRUE, main="")
```
We can see again in the plot that the density for distances $>150$m is very very small.

## Truncation

Given the poor goodness of fit results, it seems sensible to truncate the data to improve the fit of the detection function. A perennial question in distance sampling is "at what distance do I truncate?" We'd rather not throw-away too much of our (or our field workers') hard-earned data, only enough to improve the fit of our model. There is no general rule for when and where to truncate however Buckland et al [@Buckland:2001vm] suggest for line transects that a truncation is chosen such that $g(w) \approx 0.15$ (p. 103) and for point transect (due to the aforementioned geometry considerations) we choose the truncation such that $g(w) \approx 0.1$ (p. 151). These rules of thumb may or may not make sense in a given situation, for example for the pantropical dolphins we didn't truncate at the analysis stage, as it appeared that truncation had occurred during data collection (there are no "outliers" as there are for the amakihi data).

A typical procedure to ensure that the truncation used is "right" is to fit many models with different truncation and try to ensure that the truncation you choose has good goodness of fit results and few (or no) outliers.

We'll try compare two further truncation distances, one at 150m, and one at 82.5m (which was used in [@Marques:2007vm]).
```{r amakihi-truncated}
hn.df.trunc.150 <- ds(amakihi, transect="point", truncation=150)
hn.df.trunc.82.5 <- ds(amakihi, transect="point", truncation=82.5)
```
Having fitted the models, we can again compare summaries and goodness of fit, along with detection function plots.
```{r amakihi-trunc-summaries}
summary(hn.df.trunc.150)
summary(hn.df.trunc.82.5)
```
Summaries show although there is not a large change in parameter estimate between the two models, this (along with the change in truncation) does lead to a large change in the average detectability and therefore abundance estimate. Note that we can't directly compare the AICs of models with different truncations, as they use different data.

```{r amakihi-trunc-plots, results="hide", echo=FALSE, fig.width=9, fig.height=9, fig.cap="Plots of a half-normal detection function fitted to the amakihi point transect data at two different truncation distances: 150m on the left and 82.5m on the right. Top row shows the fitted detection function with histogram of distances. Bottom row shows quantile-quantile plots."}
par(mfrow=c(2,2))
plot(hn.df.trunc.150, xlab="Distance (m)", main="")
plot(hn.df.trunc.82.5, xlab="Distance (m)", main="")
gof_tests(hn.df.trunc.150)
gof_tests(hn.df.trunc.82.5)
```
We can see that the quantile-quantile plots are much improved when truncation is set at 82.5m (right side), points are closer to the line $y=x$. Though, looking at the goodness of fit test results, we don't see a very compelling story for this model.
```{r amakihi-hn-trunc-gof}
gof_tests(hn.df.trunc.82.5, plot=FALSE)
```

At this point we'll stop looking at the truncation and turn back to detection function model selection and compare the half-normal with hazard-rate model, keeping the truncation at 82.5m.
```{r amakihi-trunc-hr}
hr.df.trunc.82.5 <- ds(amakihi, transect="point", truncation=82.5, model=df(model=~hr))
summary(hr.df.trunc.82.5)
```
We can see that this model has a much better AIC than the half-normal model (hazard-rate: `r sprintf( "%.2f",hr.df.trunc.82.5$AIC)`, versus `r sprintf( "%.2f",hn.df.trunc.82.5$AIC)` for half-normal). Plotting the detection function and performing goodness of fit tests indicates that this model is much better than the half-normal.
```{r amakihi-hr-trunc-plots, echo=FALSE, fig.width=9, fig.cap="Plots of a hazard-rate detection function fitted to the amakihi point transect data. Left plot shows the fitted detection function with histogram of distances and the right shows quantile-quantile plots."}
par(mfrow=c(1,2))
plot(hr.df.trunc.82.5, xlab="Distance (m)", main="")
gof_tests(hr.df.trunc.82.5)
```

# Conclusion

This chapter has given a whirlwind tour of how detection functions are fitted in `Distance2` and how to select a good detection function. We covered both line and point transects as well as how to select the truncation distance.

In the next chapter we'll look at different models for the detection function and how we can use additional data to model detectability when it relies on other variables as well as distance.




## References



[^gomdata]: For convenience the data are bundled in an R-friendly format, although all of the code necessary for creating the data from the Distance project files is available at [github.com/dill/mexico-data](http://github.com/dill/mexico-data). The original OBIS-SEAMAP page for the data may be found at the [SEFSC GoMex Oceanic 1996 survey page](http://seamap.env.duke.edu/dataset/25).
[^amakihi]: Data are again bundled with `Distance2`. Thanks go to Steven Fancy for providing the data. A more detailed analysis is provided in the associated paper [@Marques:2007vm].
