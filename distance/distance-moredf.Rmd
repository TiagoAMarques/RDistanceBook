---
title: Improving the fit of detection functions
bibliography: full.bib
csl: biometrics.csl
---

```{r echo=FALSE, results="hide", error=FALSE, warning=FALSE, message=FALSE}
## abstract this into a header
source("../figure-captions.R")
library(Distance2)
```

The half-normal and hazard-rate detection functions seen in the previous chapters appear adequate for the datasets we've addressed so far, but there are a number of other modelling options available for the detection function. This chapter will give an overview of these models and how they can be fitted using `Distance2`. We also revisit model selection and look at the function `summarize_models`, which can build a table of fitted models before looking at common fitting problems and how to work around them.

## Additional models for the detection function

Here we'll look at two classes of detection function: *key plus adjustment terms* (K+A; also alternatively *key plus adjustment functions*) models and *mixture models*. Both of these classes fit into the AIC model selection framework discussed so far and can be tested for goodness of fit, so it's a case of understanding their formulation and how to fit them in `Distance2`.


### Key plus adjustment models

Both the half-normal and hazard-rate detection functions are somewhat limited in their flexibility. A simple adaptation of these detection functions is to consider them to be "key" functions which can be augmented with one or more "adjustment" terms [@Buckland:1992wy; @Buckland:2001vm]. The general form of these models mathematically is:
$$
g(y) = \frac{k(y)\left[ 1 + \sum_{j=2}^J \alpha_j a_j(y)\right]}{k(0)\left[ 1 + \sum_{j=2}^J \alpha_j a_j(0)\right]}
$$
where we are modelling our detection function $g(y)$ with the key function $k(y)$ (e.g. half-normal) and $J$ adjustment functions $a_j$ which are multiplied by corresponding coefficients $\alpha_j$, which we'll estimate. The denominator (which is simply the numerator evaluated at zero distance) ensures that the detection probability is certain at zero distance ($g(0)=1$). Different $a_j(y)$s are considered to be appropriate for each key function.

In general we add up to $J$ adjustments, this means that adjustments of *order* $2, \dots, J$ are included[^adjorders]. It is rarely that case that a model with more than 3 adjustment terms is selected by AIC.

Which key and adjustment combinations and the particular orders that are appropriate is decided by whether the key and adjustments are *orthogonal*. In simple terms, orthogonality mathematically describes functions that don't "overlap"; we'd like the functions not to overlap as we want each adjustment to contribute something "new" to the detection function (rather than adding to a part of the shape that's already present).

There is some controversy in the distance sampling community over whether one should include covariates in K+A models. Leaving aside the more philosophical issue of whether one *should* include covariates in a model that includes adjustments, we are left with an issue of practicality. It's highly likely that the model for the detection function will become overparameterised in this case, so in general the use of adjustments when covariates are used in the model is not encouraged. There are additional considerations covered in the [Monotonicity] and [Model selection] sections below.


#### Half-normal K+A models

When using a half-normal key, two adjustments are considered appropriate: *cosine series* and *Hermite polynomials*[^hermite]. We can look at these functions graphically:
```{r hn-adj-plots, fig.width=10, fig.cap="Half-normal key plus adjustment models. Left, with cosine adjustment terms of order 3; right, with Hermite polynomial adjustment terms of order 3. Solid line black shows the detection function, solid grey line is the half-normal key and the dashed grey show the two adjustments (scaled and shifted to be on the same axes).", echo=FALSE}
par(mfrow=c(1,2))
g.hn <- function(x,sigma=0.25, j=0, adjp=1){
  key <- exp(-x^2/(2*sigma^2))
  adj <- rep(0, length(x))
  if(j>1){
    for(adjj in 2:j){
      adj <- adj + adjp[adjj-1]*cos(adjj*pi*x)
    }
  }
  return((key*(1+adj))/(key[1]*(1+adj[1])))
}
x <- seq(0,1,len=200)

# plot half-normal with cosines superimposed
plot(x, g.hn(x, 0.6, j=3, adjp=c(0.1,0.025)), main="",
     xlab="Distance", ylab="Detection probability",
     type="l", xlim=c(0,1), ylim=c(0,1), asp=1)
lines(x, g.hn(x, 0.6, j=0), col="grey")
lines(x, (1+cos(2*pi*x))/2, lty=2, col="grey")
lines(x, (1+cos(3*pi*x))/2, lty=2, col="grey")


g.hn <- function(x,sigma=0.25, j=0, adjp=1){
  key <- exp(-x^2/(2*sigma^2))
  adj <- rep(0, length(x))
  if(j>1){
    for(adjj in 2:j){
      adj <- adj + adjp[adjj-1]*Distance2:::hermite_poly(x/sigma,2*adjj)
    }
  }
  return((key*(1+adj))/(key[1]*(1+adj[1])))
}
x <- seq(0,1,len=200)

# plot half-normal with Hermite polynomials superimposed
plot(x, g.hn(x, 0.4, j=3, adjp=c(0.01,-0.0004)), main="",
     xlab="Distance", ylab="Detection probability",
     type="l", xlim=c(0,1), ylim=c(0,1), asp=1)
lines(x, g.hn(x, 0.4, j=0), col="grey")
lines(x, 0.01*(1+Distance2:::hermite_poly(x/0.4,2*2))/(1+Distance2:::hermite_poly(0,2*2)), lty=2, col="grey")
lines(x, -0.0004*(1+Distance2:::hermite_poly(x/0.4,2*3))/(1+Distance2:::hermite_poly(0,2*3)), lty=2, col="grey")
```

These two models can be accessed in `Distance2` via the `model=` argument to `df()`. For example looking at the Gulf of Mexico dolphin data again:
```{r hn-adj-dolphins}
# half-normal with cosine adjustments of order 2
dolphins.hn_cos <- ds(mexdolphins,truncation=8000, df(model=~hn+cos(2)))
# half-normal with Hermite adjustments of order 2
dolphins.hn_hermite <- ds(mexdolphins,truncation=8000, df(model=~hn+hermite(2)))
```
Plotting these detection functions along with our half-normal model from the previous chapter, we can see that the Hermite polynomial adjustments have had no effect on the fit, where as the cosine adjustments have made the function rather too wiggly:
```{r hn-adj-dolphins-plot, fig.width=11, echo=FALSE, fig.cap="Detection function models fitted to the Mexico dolphin data, left to right: half-normal, half-normal with cosine adjustment of order 2, half-normal with Hermite polynomials of order 2."}
dolphins.hn <- ds(mexdolphins,truncation=8000, df(model=~hn))
par(mfrow=c(1,3))
plot(dolphins.hn, main="")
plot(dolphins.hn_cos, main="")
plot(dolphins.hn_hermite, main="")
```
These examples illustrate two important lessons in fitting detection function model selection.

Let's start with the half-normal with Hermite polynomial detection function, there the adjustment didn't appear to have any effect on the shape of the detection function. We can investigate the estimated parameters of the model and see that parameter that controls the influence of the Hermite adjustment is very very small:
```{r hn-adj-hermpar}
dolphins.hn_hermite$par
```
It terefore seems reasonable to rule out this model.

Second, the half-normal with cosine model begins by decreasing but then ends up increasing again before finally decreasing. We refer to this behaviour as *non-monotonic* and would prefer our detection functions to be *monotonically decreasing*; that is, as the distance from the transect increases we should see fewer animals.We'll cover this in more detail in the [Monotonicity] section, below.



#### Hazard-rate K+A models

It is also possible to combine the hazard-rate detection function with adjustment terms to lead to a more flexible fit for the detection function. We can again use cosine adjustments, just as we did with the half-normal detection function above. Instead of Hermite polynomials, we use *simple polynomials*.

Plotting an example of these two adjustments:
```{r hr-poly-plot, fig.width=10, fig.cap="Hazard-rate key with adjustments. Left, with cosine adjustments of order 3; right, simple polynomial adjustment terms of order 3. Solid line black shows the detection function, solid grey line is the hazard-rate key and the dashed grey shows the adjustment (rescaled to be on the same axes).", echo=FALSE}
par(mfrow=c(1,2))
x <- seq(0,1,len=200)
g.hr <- function(x,sigma=0.25, j=0, adjp=1,b=1){
  key <- 1 - exp(-(x/sigma)^-b)
  adj <- rep(0, length(x))
  if(j>1){
    for(adjj in 2:j){
      adj <- adj + adjp[adjj-1]*cos(adjj*pi*x)
    }
  }
  return((key*(1+adj))/(key[1]*(1+adj[1])))
}
# plot half-normal with cosine series superimposed
plot(x, g.hr(x, 0.4, j=3, adjp=c(0.3,0.2)), main="",
     xlab="Distance", ylab="Detection probability",
     type="l", xlim=c(0,1), ylim=c(0,1), asp=1)
lines(x, g.hr(x, 0.4, j=0), col="grey")
lines(x, (1+cos(2*pi*x))/2, lty=2, col="grey")
lines(x, (1+cos(3*pi*x))/2, lty=2, col="grey")


g.hr <- function(x,sigma=0.25, j=0, adjp=1,b=1){
  key <- 1 - exp(-(x/sigma)^-b)
  adj <- rep(0, length(x))
  if(j>1){
    for(adjj in 2:j){
      adj <- adj + adjp[adjj-1]*(x)^(2*adjj)
    }
  }
  return((key*(1+adj))/(key[1]*(1+adj[1])))
}

# plot half-normal with simple polynomials superimposed
plot(x, g.hr(x, 0.4, j=3, adjp=c(0.1, 0.05)), main="",
     xlab="Distance", ylab="Detection probability",
     type="l", xlim=c(0,1), ylim=c(0,1), asp=1)
lines(x, g.hr(x, 0.4, j=0), col="grey")
lines(x, 0.1*(1+(x^4)), lty=2, col="grey")
lines(x, 0.05*(1+(x^6)), lty=2, col="grey")
```

The syntax for the hazard-rate adjustment models is very similar to that of the half-normal models:
```{r hr-adj-dolphins}
# hazard-rate with a cosine adjustment of order 2
dolphins.hr_cos <- ds(mexdolphins,truncation=8000, df(model=~hr+cos(2)))
# hazard-rate with a simple polynomial adjustment of order 2
dolphins.hr_poly <- ds(mexdolphins,truncation=8000, df(model=~hr+poly(2)))
```
We can again plot the resulting models:
```{r hr-adj-dolphins-plot, fig.width=10, echo=FALSE, fig.cap="Detection function models fitted to the Mexico dolphin data, left to right: hazard-rate, half-normal with cosine adjustment of order 2, half-normal with simple polynomial of order 2."}
dolphins.hr <- ds(mexdolphins,truncation=8000, df(model=~hr))
par(mfrow=c(1,3))
plot(dolphins.hr, main="")
plot(dolphins.hr_cos, main="")
plot(dolphins.hr_poly, main="")
```
Again, we see non-monotonic fits for these data.


#### Fourier models

A final set of detection function models that fit into the "key plus adjustments" formulation is the "Fourier" type model. In this case we have a uniform key function and a cosine series as adjustments to that. We can write this model as:
$$
g(y) = \frac{\frac{1}{w}\left[ 1 + \sum_{j=1}^J \alpha_j \cos \left( \frac{j \pi y}{w} \right) \right]}{\frac{1}{w}\left[ 1 + \sum_{j=1}^J \alpha_j \cos(0)\right]}
$$
Note that here the key "function" is $1/w$ and the summation of the cosines starts at $j=1$ (rather than $j=2$ above).

We can again look at the function graphically:
```{r fourier-plot, fig.cap="Fourier detection function; uniform key with cosine adjustments of order 1 and 2. Solid line black shows the detection function, solid grey line is the constant uniform key and the dashed grey show the two adjustments (scaled and shifted to be on the same axes).", echo=FALSE}
g.hn <- function(x,sigma=0.25, j=0, adjp=1){
  key <- 1
  adj <- rep(0, length(x))
  for(adjj in 1:j){
    adj <- adj + adjp[adjj]*cos(adjj*pi*x)
  }
  return((key*(1+adj))/(key[1]*(1+adj[1])))
}
x <- seq(0,1,len=200)

# plot fourier with cosines superimposed
plot(x, g.hn(x, 0.6, j=2, adjp=c(0.4,0.2)), main="",
     xlab="Distance", ylab="Detection probability",
     type="l", xlim=c(0,1), ylim=c(0,1), asp=1)
lines(x, rep(1,length(x)), col="grey")
lines(x, (1+cos(1*pi*x))/2, lty=2, col="grey")
lines(x, (1+cos(2*pi*x))/2, lty=2, col="grey")
```
Fourier models can be specified in a similar way to the half-normal and hazard-rate models above:
```{r unif-adj-dolphins}
# uniform with a cosine adjustment of order 1
dolphins.fourier1 <- ds(mexdolphins,truncation=8000, df(model=~unif+cos(1)))
# uniform with cosine adjustments of order 1&2
dolphins.fourier12 <- ds(mexdolphins,truncation=8000, df(model=~unif+cos(1,2)))
```
We can again plot the fitted models and observe the non-monotonic detection function when cosine adjustments of order 1 and 2 are used:
```{r unif-adj-dolphins-plot, fig.width=7, echo=FALSE, fig.cap="Detection function models fitted to the Mexico dolphin data. Left: uniform with cosine adjustment of order 1. Right: uniform with cosine adjustments of order 1 and 2."}
par(mfrow=c(1,2))
plot(dolphins.fourier1, main="")
plot(dolphins.fourier12, main="")
```


#### Recap

So far we've looked at some new options for modelling the detection function. They are summarised in the following table along with their mathematical forms:

Key function   Mathematical form                                          Adjustment          Mathematical form
-------------- ---------------------------------------------------------- ------------------- ----------------------------------------
uniform        $1/w$                                                      cosine              $\sum_{j=1}^J \alpha_j \cos(j \pi y/w)$
half-normal    $\exp\left(-\frac{y^2}{2 \sigma^2}\right)$                 cosine              $\sum_{j=2}^J \alpha_j \cos(j \pi y/w)$
                                                                          Hermite polynomial  $\sum_{j=2}^J \alpha_j H_{2j}(y/w)$
hazard-rate    $1-\exp\left[-\left(\frac{y}{\sigma}\right)^{-b}\right]$   cosine              $\sum_{j=2}^J \alpha_j \cos(j \pi y/w)$
                                                                          Simple polynomial   $\sum_{j=2}^J \alpha_j (y/w)^{2j}$



#### Monotonicity

** This makes sense as we are much more likely to see animals directly in front of us than those which are further away. If fitted detection functions exhibit this kind of monotonicity then it's possible that there were problem with the survey setup (for example that transects were placed running parallel to features such as roads or hedgerows) or may simple be down to random variation in the data; either way we'd rather not include these features in our detection function. **


As noted when looking at the plots of the fitted models above, sometimes the detection function will not be monotonic over the whole of the range $(0,w)$. In this case we suspect that there has been some problem during the survey (some features that have caused animals to appear to "clump together" at a certain distance) or that the detection function is overfitting to random variation in the data (by chance there are more observations at one distance further away than others).

In either case we can find out whether and where the detection functions are non-monotonic using the `df_check_mono()` function in `Distance2`. We run the function on a fitted detection function object, if the function is monotonic then it will return `TRUE`. If the function is non-monotonic, then `df_check_mono` will return `FALSE` and will give warnings explaining how the non-monotonicity has manifested itself.

Using the last example of the Fourier type analysis to illustrate the checking function:
```{r check-fourier}
df_check_mono(dolphins.fourier1)
df_check_mono(dolphins.fourier12)
```
Note that in the latter case we can include the `plot=TRUE` argument to show where the non-monotonicity occurs:
```{r check-fourier-plot, fig.cap="Montonicity check plot for the Fourier detection function when cosine adjustments of order 1 and 2 are used. The non-monotonic part of the detection function is highlighted in red.", result="hide"}
df_check_mono(dolphins.fourier12, plot=TRUE)
```

Returning to the considerations above regarding the addition of adjustments to models which contain covariates, it's worth noting that detecting monotonicity gets much harder when covariates are included. `df_check_mono` simply evaluates $g(x)$ over a grid and checks that the points always decrease when going left to right, but for a covariate model there are effectively as many functions to check as there are different observed values of the covariates (or rather their unique combinations). This is relatively straightforward when there is a single factor with a few levels, but quickly becomes a computational nightmare.

**Fitting monotonic detection functions goes here.**

**Maybe mention here how this is a difficult optimisation problem**

### Mixture models

An alternative way to formulate the detection function is by use of *mixture models*. A mixture model takes advantage of the mathematical fact that when you sum any number of monotonic functions, the resulting function is also monotonic -- this works around the above monotonicity issues in a simple way.

CITE ME developed a mixture model detection function using a sum of half-normal functions:
$$
g(y) = \sum_{j=1}^J \phi_j \exp\left(-\frac{y^2}{2 \sigma_j^2}\right)
$$
we refer to the number of *components* or *points* in such a model and denote this $J$. Each of the $J$ components are weighted by the $\phi_j$s, which control the amount of influence that each component has in the model. We impose the condition that the $\phi_j$ must sum to one.

Plotting a mixture model detection function gives some intuition about how the model works:
```{r mixture-plot, fig.cap="Half-normal mixture model detection function. Solid line black shows the detection function, Dashed grey lines show the two the two mixture components.", echo=FALSE}
g.hn <- function(x,sigma=0.25, j=1, mixp=1){
  ss <- 0
  for(i in 1:j){
    ss <- ss + mixp[i]*exp(-x^2/(2*sigma[i]^2))
  }
  ss
}
x <- seq(0,1,len=200)

# plot half-normal with Hermite polynomials superimposed
plot(x, g.hn(x, c(0.7,0.15), j=2, mixp=c(0.5,0.5)), main="",
     xlab="Distance", ylab="Detection probability",
     type="l", xlim=c(0,1), ylim=c(0,1), asp=1)
lines(x, g.hn(x, c(0.7), j=1), lty=2, col="grey")
lines(x, g.hn(x, c(0.15), j=1), lty=2, col="grey")
```
In the above plot the two dashed lines are added (with equal weighting of $1/2$ each) to produce the bold line. Because each component is monotonic, the black line is guaranteed to be monotonic.

Since the simplest mixture model detection function uses 3 parameters, it's recommended that the formulation is used for relatively large data sets. For that reason we'll switch to the amakihi data we saw in the previous chapter to illustrate how to fit the model:
```{r hn-mix-amakihi}
data(amakihi)
# again, we'll truncate at 82.5m
# 1-point mixture -- just a half-normal model
amakihi.hn_mix1 <- ds(amakihi, transect="point", truncation=82.5, model=df(~hn+mix(1)))
# 2-point mixture
amakihi.hn_mix2 <- ds(amakihi, transect="point", truncation=82.5, model=df(~hn+mix(2)))
# 2-point mixture with observer as a covariate
amakihi.hn_mix2_obs <- ds(amakihi, transect="point", truncation=82.5, model=df(~hn+mix(2), scale=~obs))

```
We can again plot the resulting models:
```{r hn-mix-amakihi-plot, fig.width=7, echo=FALSE, fig.cap="Mixture model detection functions fitted to the Hawaiian amakihi data. Left: 1-point half-normal mixture model (equivanelent to using `~hn`). Right: 2-point mixture of half-normals."}
par(mfrow=c(1,2))
plot(amakihi.hn_mix1, main="")
plot(amakihi.hn_mix2, main="")
```

```{r hn-mix-amakihi-obs-plot, fig.width=7, echo=FALSE, fig.cap="Mixture model detection function with 2 components fitted to the Hawaiian amakihi data with observer included as a factor covariate."}
plot(amakihi.hn_mix2_obs, main=c("Average detection function","Levels of observer"))
```

Note that a "1-point" mixture is simply a half-normal detection function as descibed in [Models for detectability](distance-simpledf.html).


### Model selection

At this point we have quite a lot of models (7 for the dolphins and 3 for the amakihi) and it would be nice to return to the topic of model selection. Fortunately, all of the models described here fit into the AIC framework that we saw in the previous chapters. We can also use the goodness of fit procedures that we've seen so far.

Ignoring goodness of fit for now, we can use the `summarize_models` function in `Distance2` to create a table of model results (sorted by AIC) for easy comparison. For example, for the models of dolphins we can produce the results table by simply giving the function the models we wish to consider as arguments.
```{r dolphin-results-table}
summarize_models(dolphins.hn_cos, dolphins.hn_hermite, dolphins.hr_cos, dolphins.hr_poly, dolphins.fourier1, dolphins.fourier12)
```
Note that the numbering of rows corresponds to the order in which the models were passed to `summarize_models`. Columns are as follows:

  * `Model` gives a short description of fitted model (this may be ambiguous so the row numbers may be helpful in working out which model is which).
  * `Formula` describes the covariate model (just `~1` when there are no covariates).
  * `# pars` gives the number of parameters in the model.
  * `P_a` lists the average probability of detection.
  * `CV(P_a)` gives the coefficient of variation of average probability of detection giving an indication of uncertainty in the model (more on this in [How certain are we in our estimates?](distance-uncertainty.html)).
  * `AIC` finally lists Akaike's information criterion for the model.

We can do the same thing for the detection functions fitted to the amakihi data too:
```{r amakihi-results-table}
summarize_models(amakihi.hn_mix1, amakihi.hn_mix2, amakihi.hn_mix2_obs)
```
The `Formula` column now includes `~obs` for the model fitted with observer as a covariate.

These tables can provide a useful summary when many models have been fitted, but don't excuse the investigator from performing model checking and plotting as described previously. They are a convenient utility not a fast track to completing an analysis.

As mentioned above, we don't recommend adding adjustments to models when covariates are included, as they can lead to detection probabilities that are greater than 1 for some distances and make it difficult to diagnose monotonicity problems. However another potential problem is that in general we don't expect detection function models to have many parameters (usually models with fewer than 5 parameters are enough to encapsulate a sufficient information about the shape of the detection functions). Although the "parameter hungry" argument can also be levelled against mixture model detection functions, they at least do not suffer from the additional issues of implausible function shape.


## Model fitting problems

There are times when one can write the code to fit a model in R but it will not fit. One might receive error messages about convergence errors or invalid parameter values. These errors can happen for a variety of reasons and a comprehensive treatment is impossible but here are some possible reasons and fixes for model fitting problems.

  * **Truncation**: Model fitting an often fail if the truncation distance is too large, as there may not be a value of the model parameters that encapsulates all of the data that was collected. Decreasing the truncation distance and refitting the model may allow the model to be fitted.
  * **Too many parameters**: When including covariates in the model the number of parameters to estimate can often be large (especially when a factor with many levels is included). In this case fitting the simplest model and increasing its complexity one covariate at a time may help. One can also use the parameter values from the previous model fit as starting values for the more complex model, which at least gives the optimizer a good starting place (see `?starting_values` for more information on how to do this in `Distance2`).
  * **Optimization algorithm**: The default optimizer used by `ds` is relatively robust but it may be necessary to use additional options to allow a larger space of parameter values to be explored. A useful option is to supply the following option to `ds`: `control=list(optimx.method=c("SANN","nlminb"))` which adds `"SANN"` to the optimizers that `ds` will use (normally only `nlminb` is used). `SANN` uses simulated annealing [@Press:1990tn], which may allow us to explore more of the parameter space.
  * **Covariate scaling**: Covariates are collected on very different scales; for example Beaufort sea state will usually vary between 0 and 5, but times of day might be measured in minutes from a set point, so could be in the hundreds. These different scales can cause problems for the optimizer, so by default `ds` will attempt to rescale all the covariates by multiplying them by that covariates standard deviation divided by the standard deviation of the distances. This usually works fairly well but one might need to manually rescale in some situations. In this case the `control` option `parscale` can be set, e.g. by supplying `control=list(parscale=c(1,0.01,1)` to `ds` to perform the rescaling.

The above reasons are the most popular ways in which a model can fail to fit, though there are many other potential complex pitfalls. Building detection functions with small increments of complexity can make the diagnosis of these issues significantly easier.

It is also worth noting at this point that although convergence issues can cause the model to fail to fit entirely, it is also possible that the model will fit but the parameters which the optimizer finds are not optimal. One indication that parameters may not be optimal is large uncertainty associated with the parameters (which can be found in the model `summary` output), in this case following the above advice in setting starting values and optimizer may help.

## Recap

In this chapter we looked at additional models for the detection function that can be used in a variety of different situations when simple half-normal or hazard-rate detection functions are not flexible enough. All these models can be selected by AIC and checked for goodness of fit via Kolmogorov-Smirnov and Cramer-von Mises tests. The `summarize_models` function then allows one to easily review fitted models. Finally we looked at what can be done when models fail to fit. In the next chapter we'll look at how we can estimate abundance once we've successfully fitted and selected our detection function.



## Further reading

The models described above expand the set of possibilities considerably on those from the previous chapters. There are yet more models for the detection function to consider, though unfortunately not enough time here to give details on them all. A brief summary:

  * *Gamma* and *2-part normal* detection functions are useful when we can assume that detectability is certain at some point but that point is not at zero distance. This can happen during aerial surveys when observers struggle to see what is directly below them but can spot animals at a given distance with certainty. Gamma detection functions are covered in @Becker:2009vm.
  * *Exponential power series* is a more flexible alternative to the hazard-rate [@Otto:1990kk].
  * *Negative exponential*: can be useful when very spiked data are collected but usually is only recommended to salvage a survey gone badly wrong (as the negative exponential has no "shoulder"). A mixture model approach is usually more reliable when data are very spiked (see e.g. @Buckland:1992wy, @Buckland:2001vm, Eidous:2011ua).


## References


[^adjorders]: It doesn't really make sense to include only higher order terms, i.e. ignoring order 2 and only including order 3 rather than both 2 and 3. As the adjustment order increases the complexity of the adjustment increases and each term is orthogonal.
[^order]: Note that we refer to $j$ as the order, rather than $2j$ so we can be consistent over different adjustment functions.
[^hermite]: Further mathematical details of Hermite polynomials can be found in many textbooks, [Wolfram's page is a good starting point](http://functions.wolfram.com/Polynomials/HermiteH/02/0001/).

